\section{Introduction}

%\begin{itemize}
%	\item Motivation: Why load balancing
%	\item Context: Scenarios in which global rescheduling applies
%	\item Problem: Scaling iterative applications: algorithm limitations
%	\item Justification: Scalable load balancing, distributed rescheduling
%	\item Related work: Hierarchical (limited scaling), Distributed (excessive comm), Diffusive (iterative), Refinement Based (centralized)
%	\item Proposed solution: Batch task migration to mitigate communication costs
%	\item Paper contributions	
%	\item Results brief
%	\item Paper structure
%\end{itemize}

%Growth of systems and applications.
%Need to achieve high scalability to use platforms.
%Efficient scheduling and use of resources.
%
%Dynamic iterative applications.
%Unable to use work stealing.
%Global rescheduling as a solution.
%
%Strong scaling of applications.
%Cost of centralized load balancing.
%Using platform parallelism to achive performance.
%Communication costs as a relevant overhead.
%
%Present hierarchical LB.
%Pros: weak scaling, topology.
%Cons: high comms, bottlenecks.
%Present distributed LB.
%Pros: parallel, refinement.
%Cons: high comms, network contention.
%
%\textbf{Cut out topo aware and diffusive, not related enough.}
%
%Brief intro to BTM.
%Reduced communication.
%Highly parallel.
%Fast convergence.
%
%Present contributions.
%
%Brief results.
%
%Present paper structure.
%
%\hline

Parallel machines are at their best when work is evenly distributed among compute nodes, and idle time is merely a myth.
Unfortunately, the efficient scaling applications for these platforms has been a challenge as long as they have existed.
Uneven distribution of work and high communication overheads are the main villains when devoloping parallel applications~\cite{Deveci2015, commaware}.
Concern towards these problems increases as system grow in size, consuming more power and resources to solve problems.

Applications such as simulations of molecular dynamics~\cite{namd} and hydrodynamics~\cite{IPDPS13:LULESH} suffer from load imbalance due to their intrinsic dynamic and iterative nature.
Since the load of a simulation will vary over time, it is impossible to statically schedule work evenly if this is done only prior to application execution.
This creates an impending need for fast and effective scheduling strategies, seeking minimal overhead and maximum increase in performance.

Rescheduling algorithms have been able to take molecular dynamics from a few thousands to the hundreds of millions of atoms in the last 25 years~\cite{namd0}.
However, centralized algorithms can only take you so far.
Since mapping work to processing elements (PEs) is a NP-Hard problem~\cite{npcomplete}, the increase in application data and platform size have rapidily made algorithm overheads prohibitive in platforms of today.
This creates a need for more scalable, decentralized, rescheduling approaches, avoid both excess of data to process and network contention.

In the iterative application domain, there are two main paths to achieve scalability in global rescheduling.
The first being Hierarchical load balancing, which explores parallelism using different approaches for fine-grain and coarse-grain steps.
These can scale, but are usually tied to the same limitations as the Centralized, as data is still agregated in master nodes.
And the second is Distributed load balancing, which seeks to achieve scalability in a decentralized fashion.
These scale better, but have limited system information and may incur in high amounts of communication.

Few are the Distributed strategies in the domain of global rescheduling, but their effectiviness is notable~\cite{grapevine,diffus}.
In this paper, we present the concept of \textit{Batch Task Migration} and a novel distributed global rescheduling algorithm that applies this technique, \textit{PackDrop}.
Our approach is based on the idea of grouping tasks prior to migration decisions in batches, decreasing communication overhead in algorithm decision time and enhancing locality of migrated tasks.

Obtained results display the effectiveness of our approach in a costly communication scenario, as well as its capability to balance load and enhance application performance.
Observed application times were reduced from $13\%$ to $25\%$ when compared to an execution without load balancing in $384-768$ cores experiments.
Our approach also showed itself more than $2000$ times faster than centralized reschedulers in this same environment.

The remainder of this paper is divided as follows:
Section~\ref{sec:rw} presents recent work in dynamic rescheduling of scientific applications. 
Section~\ref{sec:algo} presents our novel approach and the developed algorithms. 
Section~\ref{sec:analysis} is a complexity analysis of \textit{PackDrop}, our distributed algorithm. 
Section~\ref{sec:impl} displays implementation details, execution environments and benchmarks used in this paper. 
Section~\ref{sec:eval} displays our performance evaluation and discussed experiments. 
And Section~\ref{sec:conclusion} presents the conclusion of this work and our future research.

%
%With the arrival of Petascale and highly parallel computing platforms, scientific simulations, such as molecular dynamics, have been able to achieve great advance in their respective fields~\cite{namd,IPDPS13:LULESH}.
%\todo[inline]{Em geral não se começa frases com "With", ainda mais a primeira do artigo. Deve-se evitar a voz passiva, a não ser quando for para quebrar a monotonia da leitura}
%However, load imbalance is a recurrent problem in scientific applications, specially in those with an intrinsic dynamic nature~\cite{Deveci2015}.
%Global rescheduling strategies try to solve this issue, redistributing work amongst Processing Elements (PEs), seeking a more homogeneous state of the system load.
%\todo[inline]{Esse primeiro parágrafo apresenta 3 conceitos ao mesmo tempo: O aumento nas plataformas, o desbalanceamento e global rescheduling mas não apresenta o estado atual e como tudo isso se liga de maneira intuitiva. Deixei comentado uma possível re-escrita.}
%
%% Modern parallel computing platforms have achieved petascale computing and will soon embark in the the hexascale era. Nonetheless, as platforms grow on computational power so does the challenge of providing application scalability, majorly due to the load imbalance problem. However, through the aid of global schedulers, it is possible to dynamically redistribute an application work amongst Processing Elements (PEs), ultimately achieving an homogeneous state of the system's load. As such, despite the aforementioned limitations, scientific simulations have been able to achieve great advance in their respective fields~\cite{namd,IPDPS13:LULESH}.
%
%The cost of dynamically rescheduling an application is bound to two main steps: i) a decision step, in which the scheduler remaps tasks to PEs; ii) a migration step, in which the data associated to the tasks are migrated~\cite{pillaphd}.
%In a small scale system, the higher cost is associated with migration costs, since communication is limited (less PEs to communicate).
%As systems scale, the communication costs associated with the decision step may considerably grow. Thus, applications may present significant performance losses when rescheduling strategies do not take into account the communication costs~\cite{grapevine}.
%
%Centralized global rescheduling strategies are very efficient when aggregating data and dealing with it \todo{Check new version}{does not create unexpectedly high migration or decision time overheads}. %was: cheap
%However, as researchers want to execute scientific applications to solve larger problems in highly parallel machines, the overhead of centralized rescheduling may be higher than the effects of load imbalance.
%This creates a need for scalable schedulers, able to present close to homogeneous distribution of load with a low overhead.
%
%Distributed rescheduling strategies use the local information available on PEs to redistribute work without global information.
%This way PEs take decisions based on a best local scenario, making most of these algorithms \textit{Greedy}.
%This is advantageous, since scheduling work amongst PEs perfectly is an NP-Hard problem~\cite{npcomplete}.
%
%As Exascale approaches, computing platforms become able of higher levels of scalability.
%Schedulers have to work with ever increasing amounts of information in these platforms, which demands higher performance and efficiency on load balancing.
%Even though few distributed strategies exist~\cite{grapevine,diffus}, it is still a field to be explored since each platform and application may benefit from different strategies.
%
%In this paper, we propose and present an evaluation of a \textit{Bin Packing} approach for refinement-based distributed global rescheduling called \textit{Task Packing}.
%Our novel strategy is based on previous distributed approaches and attempts to minimize the communication during the remapping process.
%Migrating groups of work-units instead one-by-one exchanges should mitigate most network-related issues, such as network congestion and jitter.
